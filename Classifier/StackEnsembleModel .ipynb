{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTTS1qY4Q4uD"
      },
      "outputs": [],
      "source": [
        "## Import necessary modules\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sn\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import cross_val_score,KFold,cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Dy1w1hw2Q6ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Antiviral Paper/Hybrid features-AVP.csv')"
      ],
      "metadata": {
        "id": "APTAHHgYRMQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Initialise the Scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# To scale data\n",
        "scaler.fit(df)\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "uo6UyfdKQ-pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.fillna(999, inplace=True)"
      ],
      "metadata": {
        "id": "9UfdGzIjRAYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, 0:198].values\n",
        "y = df.iloc [:, 199].values\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "i4V0dOSyRCJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "             X, y, test_size = 0.2, random_state=42)\n",
        "print (len(X_train),len(X_test),len(y_train),len(y_test))"
      ],
      "metadata": {
        "id": "6sgl6clFRD3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X, y)\n",
        "plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y, s=50, edgecolor=\"k\");\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ijVYi2a_RGlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X, y)\n",
        "plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y, s=50, edgecolor=\"k\");\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z8IsR-XfRPpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "             X, y, test_size = 0.2, random_state=42)\n",
        "print (len(X_train),len(X_test),len(y_train),len(y_test))"
      ],
      "metadata": {
        "id": "UyzMkqWPRVL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_res, y_res = sm.fit_resample(X, y)\n",
        "print( X_res.shape)\n",
        "print( y_res.shape)\n",
        "plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y, s=50, edgecolor=\"k\");"
      ],
      "metadata": {
        "id": "LaMwoj2zRXek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "             X_res, y_res, test_size = 0.2, random_state=42)\n",
        "print (len(X_train),len(X_test),len(y_train),len(y_test))"
      ],
      "metadata": {
        "id": "uMOGh8AVRZRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "w6YFMRHCRb5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra Tree Classifier"
      ],
      "metadata": {
        "id": "vHriXIzXRfuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "et = ExtraTreesClassifier(n_estimators=100,criterion='entropy',max_features='sqrt', random_state=5)  # Define classifier\n",
        "et.fit(X_train, y_train)  # Train model\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = et.predict(X_train)\n",
        "y_test_pred = et.predict(X_test)\n",
        "\n",
        "# Test set performance\n",
        "et_test_accuracy = accuracy_score(y_test, y_test_pred)  # Calculate Accuracy\n",
        "et_test_mcc = matthews_corrcoef(y_test, y_test_pred)  # Calculate MCC\n",
        "et_test_f1 = f1_score(y_test, y_test_pred, average='weighted')  # Calculate F1-score\n",
        "et_test_confusion = confusion_matrix(y_test, y_test_pred)  # Confusion matrix\n",
        "et_test_sensitivity = et_test_confusion[1, 1] / (et_test_confusion[1, 1] + et_test_confusion[1, 0])  # Sensitivity (True Positive Rate)\n",
        "et_test_specificity = et_test_confusion[0, 0] / (et_test_confusion[0, 0] + et_test_confusion[0, 1])  # Specificity (True Negative Rate)\n",
        "et_test_auc = roc_auc_score(y_test, et.predict_proba(X_test)[:, 1])  # Calculate AUC\n",
        "\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % et_test_accuracy)\n",
        "print('- MCC: %s' % et_test_mcc)\n",
        "print('- F1 score: %s' % et_test_f1)\n",
        "print('- Sensitivity: %s' % et_test_sensitivity)\n",
        "print('- Specificity: %s' % et_test_specificity)\n",
        "print('- AUC: %s' % et_test_auc)\n"
      ],
      "metadata": {
        "id": "csIR35e0Rhx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier"
      ],
      "metadata": {
        "id": "rXEymIT1Rjl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100,random_state=5) # Define classifier\n",
        "rf.fit(X_train, y_train) # Train model\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = rf.predict(X_train)\n",
        "y_test_pred = rf.predict(X_test)\n",
        "\n",
        "\n",
        "# Test set performance\n",
        "rf_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "rf_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "rf_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "rf_test_confusion = confusion_matrix(y_test, y_test_pred) # Confusion matrix\n",
        "rf_test_sensitivity = rf_test_confusion[1, 1] / (rf_test_confusion[1, 1] + rf_test_confusion[1, 0]) # Sensitivity (True Positive Rate)\n",
        "rf_test_specificity = rf_test_confusion[0, 0] / (rf_test_confusion[0, 0] + rf_test_confusion[0, 1]) # Specificity (True Negative Rate)\n",
        "rf_test_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1]) # Calculate AUC\n",
        "\n",
        "\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % rf_test_accuracy)\n",
        "print('- MCC: %s' % rf_test_mcc)\n",
        "print('- F1 score: %s' % rf_test_f1)\n",
        "print('- Sensitivity: %s' % rf_test_sensitivity)\n",
        "print('- Specificity: %s' % rf_test_specificity)\n",
        "print('- AUC: %s' % rf_test_auc)\n"
      ],
      "metadata": {
        "id": "QlS1zlkRRmL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNN Classifier"
      ],
      "metadata": {
        "id": "0VXxavVpRn3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Define the deep neural network (DNN) model\n",
        "model = MLPClassifier(hidden_layer_sizes=(128, 64, 32, 16),  # Four hidden layers\n",
        "                         alpha=0, max_iter=500,activation='relu',solver='adam',\n",
        "                      learning_rate_init=0.001,early_stopping=False, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Test set performance\n",
        "model_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "model_test_mcc = matthews_corrcoef(y_test, y_test_pred)\n",
        "model_test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
        "model_test_confusion = confusion_matrix(y_test, y_test_pred)\n",
        "model_test_sensitivity = model_test_confusion[1, 1] / (model_test_confusion[1, 1] + model_test_confusion[1, 0])\n",
        "model_test_specificity = model_test_confusion[0, 0] / (model_test_confusion[0, 0] + model_test_confusion[0, 1])\n",
        "model_test_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "\n",
        "print('DNN Model performance for Test set')\n",
        "print('- Accuracy: %s' % model_test_accuracy)\n",
        "print('- MCC: %s' % model_test_mcc)\n",
        "print('- F1 score: %s' % model_test_f1)\n",
        "print('- Sensitivity: %s' % model_test_sensitivity)\n",
        "print('- Specificity: %s' % model_test_specificity)\n",
        "print('- AUC: %s' % model_test_auc)\n"
      ],
      "metadata": {
        "id": "ej83nTACRqcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGB Classifier"
      ],
      "metadata": {
        "id": "sNmYcRmERr9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "xgb = XGBClassifier(n_estimators=100, max_depth=2,gamma= 0.1,booster='gbtree',random_state=5)  # Define XGBoost classifier\n",
        "xgb.fit(X_train, y_train)  # Train model\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = xgb.predict(X_train)\n",
        "y_test_pred = xgb.predict(X_test)\n",
        "\n",
        "\n",
        "# Test set performance\n",
        "xgb_test_accuracy = accuracy_score(y_test, y_test_pred)  # Calculate Accuracy\n",
        "xgb_test_mcc = matthews_corrcoef(y_test, y_test_pred)  # Calculate MCC\n",
        "xgb_test_f1 = f1_score(y_test, y_test_pred, average='weighted')  # Calculate F1-score\n",
        "xgb_test_confusion = confusion_matrix(y_test, y_test_pred)  # Confusion matrix\n",
        "xgb_test_sensitivity = xgb_test_confusion[1, 1] / (xgb_test_confusion[1, 1] + xgb_test_confusion[1, 0])  # Sensitivity (True Positive Rate)\n",
        "xgb_test_specificity = xgb_test_confusion[0, 0] / (xgb_test_confusion[0, 0] + xgb_test_confusion[0, 1])  # Specificity (True Negative Rate)\n",
        "xgb_test_auc = roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1])  # Calculate AUC\n",
        "\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % xgb_test_accuracy)\n",
        "print('- MCC: %s' % xgb_test_mcc)\n",
        "print('- F1 score: %s' % xgb_test_f1)\n",
        "print('- Sensitivity: %s' % xgb_test_sensitivity)\n",
        "print('- Specificity: %s' % xgb_test_specificity)\n",
        "print('- AUC: %s' % xgb_test_auc)\n"
      ],
      "metadata": {
        "id": "JrqROCC4RuQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Stacking"
      ],
      "metadata": {
        "id": "a5cKTkbKRxrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Define estimators\n",
        "estimator_list = [\n",
        "    ('dnn', model),\n",
        "    ('xgb', xgb),\n",
        "    ('et', et),\n",
        "    ('rf', rf)\n",
        "]\n",
        "\n",
        "# Build stack model\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=estimator_list, final_estimator=LogisticRegression(), stack_method='predict_proba'\n",
        ")\n",
        "\n",
        "# Train stacked model\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_prob = stack_model.predict_proba(X_train)\n",
        "y_test_prob = stack_model.predict_proba(X_test)\n",
        "\n",
        "# Obtain the predicted class based on probabilities\n",
        "y_train_pred = stack_model.classes_[y_train_prob.argmax(axis=1)]\n",
        "y_test_pred = stack_model.classes_[y_test_prob.argmax(axis=1)]\n",
        "\n",
        "\n",
        "# Test set model performance\n",
        "stack_model_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
        "stack_model_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
        "stack_model_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
        "stack_model_test_confusion = confusion_matrix(y_test, y_test_pred) # Confusion matrix\n",
        "stack_model_test_sensitivity = stack_model_test_confusion[1, 1] / (stack_model_test_confusion[1, 1] + stack_model_test_confusion[1, 0]) # Sensitivity (True Positive Rate)\n",
        "stack_model_test_specificity = stack_model_test_confusion[0, 0] / (stack_model_test_confusion[0, 0] + stack_model_test_confusion[0, 1]) # Specificity (True Negative Rate)\n",
        "stack_model_test_auc = roc_auc_score(y_test, y_test_prob[:, 1]) # Calculate AUC\n",
        "\n",
        "print('Model performance for Test set')\n",
        "print('- Accuracy: %s' % stack_model_test_accuracy)\n",
        "print('- MCC: %s' % stack_model_test_mcc)\n",
        "print('- F1 score: %s' % stack_model_test_f1)\n",
        "print('- Sensitivity: %s' % stack_model_test_sensitivity)\n",
        "print('- Specificity: %s' % stack_model_test_specificity)\n",
        "print('- AUC: %s' % stack_model_test_auc)\n"
      ],
      "metadata": {
        "id": "AiyqbQ8fRyox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_test_list = {'xgb':xgb_test_accuracy,\n",
        "'dnn': model_test_accuracy,\n",
        "'et': et_test_accuracy,\n",
        "'rf': rf_test_accuracy,\n",
        "'stack': stack_model_test_accuracy}\n",
        "\n",
        "mcc_test_list = {'xgb':xgb_test_mcc,\n",
        "'dnn': model_test_mcc,\n",
        "'et': et_test_mcc,\n",
        "'rf': rf_test_mcc,\n",
        "'stack': stack_model_test_mcc}\n",
        "\n",
        "f1_test_list = {'xgb':xgb_test_f1,\n",
        "'dnn': model_test_f1,\n",
        "'et': et_test_f1,\n",
        "'rf': rf_test_f1,\n",
        "'stack': stack_model_test_f1}\n",
        "\n",
        "sen_test_list = {'xgb':xgb_test_sensitivity,\n",
        "'dnn': model_test_sensitivity,\n",
        "'et': et_test_sensitivity,\n",
        "'rf': rf_test_sensitivity,\n",
        "'stack': stack_model_test_sensitivity}\n",
        "\n",
        "sps_test_list = {'xgb':xgb_test_specificity,\n",
        "'dnn': model_test_specificity,\n",
        "'et': et_test_specificity,\n",
        "'rf': rf_test_specificity,\n",
        "'stack': stack_model_test_specificity}\n",
        "\n",
        "aucroc_test_list = {'xgb':xgb_test_auc,\n",
        "'dnn': model_test_auc,\n",
        "'et': et_test_auc,\n",
        "'rf': rf_test_auc,\n",
        "'stack': stack_model_test_auc}"
      ],
      "metadata": {
        "id": "txw4XSGhRdbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "acc_df = pd.DataFrame.from_dict(acc_test_list, orient='index', columns=['Accuracy'])\n",
        "mcc_df = pd.DataFrame.from_dict(mcc_test_list, orient='index', columns=['MCC'])\n",
        "f1_df = pd.DataFrame.from_dict(f1_test_list, orient='index', columns=['F1'])\n",
        "sens_df = pd.DataFrame.from_dict(sen_test_list, orient='index', columns=['sensitivity'])\n",
        "spe_df = pd.DataFrame.from_dict(sps_test_list, orient='index', columns=['specificity'])\n",
        "aucs_df = pd.DataFrame.from_dict(aucroc_test_list, orient='index', columns=['AUC-ROC'])\n",
        "df = pd.concat([acc_df, mcc_df, f1_df,sens_df,spe_df,aucs_df], axis=1)\n",
        "df"
      ],
      "metadata": {
        "id": "0lE-u69CR3xo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}